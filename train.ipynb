{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOlye2W7gZ3Wzf5541A3Hgs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rc4yGL55HuEI","executionInfo":{"status":"ok","timestamp":1734795427330,"user_tz":-420,"elapsed":3955,"user":{"displayName":"Minh Khải","userId":"17162546248965680877"}},"outputId":"50c187ee-6b0a-4f1e-e94e-4808304a5149"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/RL/MARL-Project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ei3gU2euIgcu","executionInfo":{"status":"ok","timestamp":1734795427330,"user_tz":-420,"elapsed":2,"user":{"displayName":"Minh Khải","userId":"17162546248965680877"}},"outputId":"cd4b9a32-7a3c-451d-8322-2e25cc1719e4"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/RL/MARL-Project\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8F2Gec8HIhBQ","executionInfo":{"status":"ok","timestamp":1734795441223,"user_tz":-420,"elapsed":13894,"user":{"displayName":"Minh Khải","userId":"17162546248965680877"}},"outputId":"41847033-3dd1-493a-c818-34693d23d45d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/Farama-Foundation/MAgent2 (from -r requirements.txt (line 1))\n","  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-_casct8r\n","  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-_casct8r\n","  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.10.0.84)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.5.1+cu121)\n","Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3->-r requirements.txt (line 1)) (1.26.4)\n","Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3->-r requirements.txt (line 1)) (2.6.1)\n","Requirement already satisfied: pettingzoo>=1.23.1 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo>=1.23.1->magent2==0.3.3->-r requirements.txt (line 1)) (1.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (3.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3->-r requirements.txt (line 1)) (3.1.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3->-r requirements.txt (line 1)) (0.0.4)\n"]}]},{"cell_type":"code","source":["!python trainer.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFk-QGgzIgZx","executionInfo":{"status":"ok","timestamp":1734796186616,"user_tz":-420,"elapsed":601788,"user":{"displayName":"Minh Khải","userId":"17162546248965680877"}},"outputId":"416009ec-2e03-46d4-baa7-1196618acb92"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/RL/MARL-Project/trainer.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n","/content/drive/MyDrive/RL/MARL-Project/trainer.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n","/content/drive/MyDrive/RL/MARL-Project/trainer.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n","Episode 0, Epsilon: 1.00, Total Reward: -3296.570120178163, Steps: 161647, Max Reward: -33.80000144056976, Avg Reward: -40.70\n","Episode 1, Epsilon: 0.97, Total Reward: -3106.7701125973836, Steps: 158709, Max Reward: -5.8800002140924335, Avg Reward: -38.36\n","Episode 2, Epsilon: 0.94, Total Reward: -3148.4751133229584, Steps: 161846, Max Reward: -32.000001307576895, Avg Reward: -38.87\n","Episode 3, Epsilon: 0.91, Total Reward: -2998.2751075793058, Steps: 158958, Max Reward: -2.6150001185014844, Avg Reward: -37.02\n","Episode 4, Epsilon: 0.89, Total Reward: -2969.7201053388417, Steps: 160277, Max Reward: -3.5050001479685307, Avg Reward: -36.66\n","Episode 5, Epsilon: 0.86, Total Reward: -2856.185101156123, Steps: 160580, Max Reward: -10.100000417791307, Avg Reward: -35.26\n","Episode 6, Epsilon: 0.83, Total Reward: -2785.610097769648, Steps: 161128, Max Reward: -22.51000080164522, Avg Reward: -34.39\n","Episode 7, Epsilon: 0.81, Total Reward: -2717.895094995387, Steps: 160386, Max Reward: -10.035000363364816, Avg Reward: -33.55\n","Episode 8, Epsilon: 0.78, Total Reward: -2529.245088289492, Steps: 157793, Max Reward: -5.115000174380839, Avg Reward: -31.23\n","Episode 9, Epsilon: 0.76, Total Reward: -2313.330084623769, Steps: 153787, Max Reward: -3.7950003473088145, Avg Reward: -28.56\n","Episode 10, Epsilon: 0.74, Total Reward: -2197.925085180439, Steps: 146630, Max Reward: -3.7250001709908247, Avg Reward: -27.13\n","Episode 11, Epsilon: 0.72, Total Reward: -1872.2000806340948, Steps: 116237, Max Reward: 2.1799998562783003, Avg Reward: -23.11\n","Episode 12, Epsilon: 0.69, Total Reward: -731.1350415823981, Steps: 56641, Max Reward: 14.409999337978661, Avg Reward: -9.03\n","Episode 13, Epsilon: 0.67, Total Reward: -534.5150343775749, Steps: 48829, Max Reward: 23.914999372325838, Avg Reward: -6.60\n","Episode 14, Epsilon: 0.65, Total Reward: -1936.2100780038163, Steps: 95113, Max Reward: 13.18999985139817, Avg Reward: -23.90\n","Episode 15, Epsilon: 0.63, Total Reward: -142.39502116665244, Steps: 30756, Max Reward: 23.699999613687396, Avg Reward: -1.76\n","Episode 16, Epsilon: 0.61, Total Reward: -150.9350209981203, Steps: 29254, Max Reward: 28.884999663569033, Avg Reward: -1.86\n","Episode 17, Epsilon: 0.60, Total Reward: -12.40001635812223, Steps: 25709, Max Reward: 31.01999966148287, Avg Reward: -0.15\n","Episode 18, Epsilon: 0.58, Total Reward: 25.23998500034213, Steps: 22343, Max Reward: 21.339999792166054, Avg Reward: 0.31\n","Episode 19, Epsilon: 0.56, Total Reward: -18.790016126818955, Steps: 23972, Max Reward: 24.80999971460551, Avg Reward: -0.23\n","Episode 20, Epsilon: 0.54, Total Reward: 39.55498562287539, Steps: 22113, Max Reward: 16.7299997350201, Avg Reward: 0.49\n","Episode 21, Epsilon: 0.53, Total Reward: -1613.3000631695613, Steps: 88198, Max Reward: -4.600000943988562, Avg Reward: -19.92\n","Episode 22, Epsilon: 0.51, Total Reward: 180.4949901420623, Steps: 15869, Max Reward: 22.96499979030341, Avg Reward: 2.23\n","Episode 23, Epsilon: 0.50, Total Reward: 246.99999232310802, Steps: 12363, Max Reward: 24.09999980777502, Avg Reward: 3.05\n","Episode 24, Epsilon: 0.48, Total Reward: 239.3049919726327, Steps: 12459, Max Reward: 18.104999841190875, Avg Reward: 2.95\n","Episode 25, Epsilon: 0.47, Total Reward: 217.4049915401265, Steps: 13606, Max Reward: 22.90499984100461, Avg Reward: 2.68\n","Episode 26, Epsilon: 0.45, Total Reward: 204.6799911269918, Steps: 14110, Max Reward: 23.279999836347997, Avg Reward: 2.53\n","Episode 27, Epsilon: 0.44, Total Reward: -1327.0000510774553, Steps: 86072, Max Reward: 3.399999303743243, Avg Reward: -16.38\n","Episode 28, Epsilon: 0.43, Total Reward: 319.84499471168965, Steps: 9459, Max Reward: 23.644999817945063, Avg Reward: 3.95\n","Episode 29, Epsilon: 0.41, Total Reward: 293.77999395504594, Steps: 10290, Max Reward: 28.47999982815236, Avg Reward: 3.63\n","Episode 30, Epsilon: 0.40, Total Reward: 291.84999385941774, Steps: 10922, Max Reward: 19.669999870471656, Avg Reward: 3.60\n","Episode 31, Epsilon: 0.39, Total Reward: 259.44999289885163, Steps: 12526, Max Reward: 18.084999880753458, Avg Reward: 3.20\n","Episode 32, Epsilon: 0.38, Total Reward: 175.43999071605504, Steps: 17186, Max Reward: 17.004999830387533, Avg Reward: 2.17\n","Episode 33, Epsilon: 0.37, Total Reward: 266.6299932943657, Steps: 12552, Max Reward: 14.064999888651073, Avg Reward: 3.29\n","Episode 34, Epsilon: 0.36, Total Reward: 60.069988113828, Steps: 24430, Max Reward: 21.769999772310257, Avg Reward: 0.74\n","Episode 35, Epsilon: 0.34, Total Reward: -898.6950360210612, Steps: 77832, Max Reward: 10.299999482929707, Avg Reward: -11.10\n","Episode 36, Epsilon: 0.33, Total Reward: 213.43999210186303, Steps: 17463, Max Reward: 32.139999750070274, Avg Reward: 2.64\n","Episode 37, Epsilon: 0.32, Total Reward: 226.60499244276434, Steps: 15892, Max Reward: 29.28999981470406, Avg Reward: 2.80\n","Episode 38, Epsilon: 0.31, Total Reward: -739.170031263493, Steps: 71717, Max Reward: 15.529999505728483, Avg Reward: -9.13\n","Episode 39, Epsilon: 0.30, Total Reward: 186.9699914464727, Steps: 19360, Max Reward: 33.62499977089465, Avg Reward: 2.31\n","Episode 40, Epsilon: 0.30, Total Reward: 79.76998907048255, Steps: 25993, Max Reward: 21.559999759308994, Avg Reward: 0.98\n","Episode 41, Epsilon: 0.29, Total Reward: 83.14998932089657, Steps: 25862, Max Reward: 25.194999711588025, Avg Reward: 1.03\n","Episode 42, Epsilon: 0.28, Total Reward: 221.63999261055142, Steps: 17719, Max Reward: 47.29499968700111, Avg Reward: 2.74\n","Episode 43, Epsilon: 0.27, Total Reward: 298.32499426137656, Steps: 11892, Max Reward: 18.624999867752194, Avg Reward: 3.68\n","Episode 44, Epsilon: 0.26, Total Reward: 292.3149941461161, Steps: 12353, Max Reward: 18.49999986961484, Avg Reward: 3.61\n","Episode 45, Epsilon: 0.25, Total Reward: 284.6649940600619, Steps: 13297, Max Reward: 23.54999984893948, Avg Reward: 3.51\n","Episode 46, Epsilon: 0.25, Total Reward: 279.05499388370663, Steps: 12909, Max Reward: 28.954999834299088, Avg Reward: 3.45\n","Episode 47, Epsilon: 0.24, Total Reward: 312.83999471273273, Steps: 11064, Max Reward: 19.15999987721443, Avg Reward: 3.86\n","Early stopping at episode 48\n","Thời gian thực thi: 593.296896 giây\n"]}]},{"cell_type":"code","source":["!python eval.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AV4N2k0EIgW5","executionInfo":{"status":"ok","timestamp":1734796716175,"user_tz":-420,"elapsed":529561,"user":{"displayName":"Minh Khải","userId":"17162546248965680877"}},"outputId":"f825205f-338f-4ef9-bbe5-10115a7c696e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["====================\n","Eval with random policy\n","/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/conversions.py:289: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n","  warnings.warn(\n","  0% 0/30 [00:00<?, ?it/s]Done recording pretrained agents\n","  3% 1/30 [00:06<02:54,  6.02s/it]Done recording pretrained agents\n","  7% 2/30 [00:13<03:13,  6.91s/it]Done recording pretrained agents\n"," 10% 3/30 [00:17<02:35,  5.76s/it]Done recording pretrained agents\n"," 13% 4/30 [00:23<02:28,  5.73s/it]Done recording pretrained agents\n"," 17% 5/30 [00:27<02:10,  5.21s/it]Done recording pretrained agents\n"," 20% 6/30 [00:38<02:46,  6.92s/it]Done recording pretrained agents\n"," 23% 7/30 [00:44<02:33,  6.68s/it]Done recording pretrained agents\n"," 27% 8/30 [00:48<02:07,  5.78s/it]Done recording pretrained agents\n"," 30% 9/30 [00:55<02:09,  6.19s/it]Done recording pretrained agents\n"," 33% 10/30 [00:59<01:54,  5.71s/it]Done recording pretrained agents\n"," 37% 11/30 [01:05<01:49,  5.76s/it]Done recording pretrained agents\n"," 40% 12/30 [01:10<01:35,  5.32s/it]Done recording pretrained agents\n"," 43% 13/30 [01:18<01:44,  6.15s/it]Done recording pretrained agents\n"," 47% 14/30 [01:22<01:30,  5.68s/it]Done recording pretrained agents\n"," 50% 15/30 [01:30<01:35,  6.35s/it]Done recording pretrained agents\n"," 53% 16/30 [01:36<01:24,  6.07s/it]Done recording pretrained agents\n"," 57% 17/30 [01:40<01:13,  5.63s/it]Done recording pretrained agents\n"," 60% 18/30 [01:46<01:07,  5.59s/it]Done recording pretrained agents\n"," 63% 19/30 [01:51<01:00,  5.50s/it]Done recording pretrained agents\n"," 67% 20/30 [01:57<00:55,  5.55s/it]Done recording pretrained agents\n"," 70% 21/30 [02:02<00:49,  5.55s/it]Done recording pretrained agents\n"," 73% 22/30 [02:08<00:45,  5.70s/it]Done recording pretrained agents\n"," 77% 23/30 [02:14<00:39,  5.59s/it]Done recording pretrained agents\n"," 80% 24/30 [02:20<00:34,  5.69s/it]Done recording pretrained agents\n"," 83% 25/30 [02:24<00:26,  5.38s/it]Done recording pretrained agents\n"," 87% 26/30 [02:30<00:22,  5.52s/it]Done recording pretrained agents\n"," 90% 27/30 [02:34<00:15,  5.18s/it]Done recording pretrained agents\n"," 93% 28/30 [02:41<00:10,  5.48s/it]Done recording pretrained agents\n"," 97% 29/30 [02:45<00:05,  5.07s/it]Done recording pretrained agents\n","100% 30/30 [02:51<00:00,  5.71s/it]\n","{'winrate_red': 0.0, 'winrate_blue': 1.0, 'average_rewards_red': -1.144090576258722, 'average_rewards_blue': 4.902213966128238}\n","====================\n","Eval with trained policy\n","/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/conversions.py:289: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n","  warnings.warn(\n","  0% 0/30 [00:00<?, ?it/s]Done recording pretrained agents\n","  3% 1/30 [00:05<02:36,  5.40s/it]Done recording pretrained agents\n","  7% 2/30 [00:10<02:21,  5.07s/it]Done recording pretrained agents\n"," 10% 3/30 [00:15<02:19,  5.15s/it]Done recording pretrained agents\n"," 13% 4/30 [00:19<02:06,  4.86s/it]Done recording pretrained agents\n"," 17% 5/30 [00:24<02:01,  4.85s/it]Done recording pretrained agents\n"," 20% 6/30 [00:30<02:04,  5.20s/it]Done recording pretrained agents\n"," 23% 7/30 [00:34<01:53,  4.91s/it]Done recording pretrained agents\n"," 27% 8/30 [00:39<01:46,  4.85s/it]Done recording pretrained agents\n"," 30% 9/30 [00:44<01:40,  4.78s/it]Done recording pretrained agents\n"," 33% 10/30 [00:49<01:38,  4.95s/it]Done recording pretrained agents\n"," 37% 11/30 [00:54<01:34,  4.98s/it]Done recording pretrained agents\n"," 40% 12/30 [00:59<01:28,  4.94s/it]Done recording pretrained agents\n"," 43% 13/30 [01:03<01:20,  4.76s/it]Done recording pretrained agents\n"," 47% 14/30 [01:09<01:21,  5.07s/it]Done recording pretrained agents\n"," 50% 15/30 [01:13<01:12,  4.82s/it]Done recording pretrained agents\n"," 53% 16/30 [01:18<01:07,  4.81s/it]Done recording pretrained agents\n"," 57% 17/30 [01:23<01:02,  4.79s/it]Done recording pretrained agents\n"," 60% 18/30 [01:28<00:57,  4.76s/it]Done recording pretrained agents\n"," 63% 19/30 [01:33<00:55,  5.04s/it]Done recording pretrained agents\n"," 67% 20/30 [01:38<00:49,  4.92s/it]Done recording pretrained agents\n"," 70% 21/30 [01:43<00:43,  4.86s/it]Done recording pretrained agents\n"," 73% 22/30 [01:48<00:39,  4.97s/it]Done recording pretrained agents\n"," 77% 23/30 [01:52<00:33,  4.84s/it]Done recording pretrained agents\n"," 80% 24/30 [01:57<00:28,  4.79s/it]Done recording pretrained agents\n"," 83% 25/30 [02:02<00:24,  4.85s/it]Done recording pretrained agents\n"," 87% 26/30 [02:06<00:18,  4.64s/it]Done recording pretrained agents\n"," 90% 27/30 [02:11<00:14,  4.76s/it]Done recording pretrained agents\n"," 93% 28/30 [02:16<00:09,  4.76s/it]Done recording pretrained agents\n"," 97% 29/30 [02:20<00:04,  4.63s/it]Done recording pretrained agents\n","100% 30/30 [02:26<00:00,  4.89s/it]\n","{'winrate_red': 0.0, 'winrate_blue': 1.0, 'average_rewards_red': 0.6164464942766009, 'average_rewards_blue': 4.999339478644217}\n","====================\n","Eval with final trained policy\n","  0% 0/30 [00:00<?, ?it/s]Done recording pretrained agents\n","  3% 1/30 [00:05<02:48,  5.81s/it]Done recording pretrained agents\n","  7% 2/30 [00:14<03:24,  7.29s/it]Done recording pretrained agents\n"," 10% 3/30 [00:20<03:11,  7.09s/it]Done recording pretrained agents\n"," 13% 4/30 [00:28<03:06,  7.16s/it]Done recording pretrained agents\n"," 17% 5/30 [00:34<02:49,  6.78s/it]Done recording pretrained agents\n"," 20% 6/30 [00:42<02:55,  7.31s/it]Done recording pretrained agents\n"," 23% 7/30 [00:48<02:37,  6.85s/it]Done recording pretrained agents\n"," 27% 8/30 [00:55<02:32,  6.93s/it]Done recording pretrained agents\n"," 30% 9/30 [01:04<02:37,  7.48s/it]Done recording pretrained agents\n"," 33% 10/30 [01:11<02:27,  7.38s/it]Done recording pretrained agents\n"," 37% 11/30 [01:17<02:12,  6.95s/it]Done recording pretrained agents\n"," 40% 12/30 [01:24<02:04,  6.89s/it]Done recording pretrained agents\n"," 43% 13/30 [01:32<02:01,  7.15s/it]Done recording pretrained agents\n"," 47% 14/30 [01:40<01:58,  7.41s/it]Done recording pretrained agents\n"," 50% 15/30 [01:47<01:50,  7.37s/it]Done recording pretrained agents\n"," 53% 16/30 [01:53<01:38,  7.05s/it]Done recording pretrained agents\n"," 57% 17/30 [02:00<01:31,  7.03s/it]Done recording pretrained agents\n"," 60% 18/30 [02:06<01:21,  6.80s/it]Done recording pretrained agents\n"," 63% 19/30 [02:14<01:18,  7.10s/it]Done recording pretrained agents\n"," 67% 20/30 [02:20<01:06,  6.66s/it]Done recording pretrained agents\n"," 70% 21/30 [02:28<01:04,  7.13s/it]Done recording pretrained agents\n"," 73% 22/30 [02:33<00:52,  6.60s/it]Done recording pretrained agents\n"," 77% 23/30 [02:42<00:49,  7.08s/it]Done recording pretrained agents\n"," 80% 24/30 [02:48<00:41,  6.92s/it]Done recording pretrained agents\n"," 83% 25/30 [02:55<00:34,  6.97s/it]Done recording pretrained agents\n"," 87% 26/30 [03:01<00:26,  6.67s/it]Done recording pretrained agents\n"," 90% 27/30 [03:09<00:20,  6.99s/it]Done recording pretrained agents\n"," 93% 28/30 [03:15<00:13,  6.66s/it]Done recording pretrained agents\n"," 97% 29/30 [03:23<00:06,  6.99s/it]Done recording pretrained agents\n","100% 30/30 [03:29<00:00,  6.98s/it]\n","{'winrate_red': 0.0, 'winrate_blue': 0.9666666666666667, 'average_rewards_red': 2.3925987470320167, 'average_rewards_blue': 4.880810672350596}\n","====================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"z5cD2XvbU897"},"execution_count":null,"outputs":[]}]}